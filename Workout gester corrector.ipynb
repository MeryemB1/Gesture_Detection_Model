{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e96736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#principale missions : detect image \n",
    "#undestand gesture \n",
    "#provide ways to correct it \n",
    "# mission 1 classify gesture from the four principale exercices ---> add data \n",
    "# Squat , Push Up , Planck,Burpees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa71a89",
   "metadata": {},
   "source": [
    "# Part One : Workout Exercice classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "types = os.listdir('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\train')\n",
    "print (types)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e95214",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a13b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for item in types:\n",
    " # Get all the file names\n",
    " sub_files = os.listdir('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\train' + '\\\\' +item)\n",
    " # Add them to the list\n",
    " for exe in sub_files:\n",
    "    items.append((item, str('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\train' + '\\\\' +item) + '\\\\' + exe))\n",
    "train_df = pd.DataFrame(data=items, columns=['exercice_type', 'video_name'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())\n",
    "df = train_df.loc[:,['video_name','exercice_type']]\n",
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29742e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "IMG_SIZE = 224\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "            if len(frames) == max_frames:\n",
    "             break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f690b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"exercice_type\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "labels = train_df[\"exercice_type\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1cf3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    \n",
    "    labels = df[\"exercice_type\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  \n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "print(f\"train_labels in train set: {train_labels.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c745fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.listdir('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\test')\n",
    "print(dataset_path)\n",
    "\n",
    "room_types = os.listdir('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\test')\n",
    "print(\"Types of activities found: \", len(dataset_path))\n",
    "\n",
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\test' + '\\\\' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('C:\\\\Users\\\\km_ka\\\\Desktop\\\\workout\\\\test' + '\\\\' +item) + '\\\\' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "test_df = pd.DataFrame(data=rooms, columns=['exercice_type', 'video_name'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "\n",
    "df = test_df.loc[:,['video_name','exercice_type']]\n",
    "df\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e61780",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f505c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "print(f\"test_labels in train set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = r'C:\\Users\\km_ka\\Desktop\\workout\\test\\squat\\pexels-mart-production-8837221 (2160p).mp4'\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ce953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
    "        <source src='C:/Users/km_ka/Desktop/workout/test/squat/pexels-mart-production-8837221 (2160p).mp4' type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8a096",
   "metadata": {},
   "source": [
    "# Part two : tracking poses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906806",
   "metadata": {},
   "source": [
    "### VIDEO FEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abea0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing=mp.solutions.drawing_utils\n",
    "pose=mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80391457",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read();\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020df7ea",
   "metadata": {},
   "source": [
    "### Make detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c98f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle (a,b,c):\n",
    "    a=np.array(a)\n",
    "    b=np.array(b)\n",
    "    c=np.array(c)\n",
    "    radian=np.arctan2(c[1]-b[1],c[0]-b[0])-np.arctan2(a[1]-b[1],a[0]-b[0]);\n",
    "    angle=np.abs(radian*180.0/np.pi);\n",
    "    if angle>180.0:\n",
    "        angle=360-angle;\n",
    "    return(angle)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ed740",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "with pose.Pose(min_detection_confidence = 0.5,min_tracking_confidence=0.5) as poset:\n",
    "  while cap.isOpened():\n",
    "    ret, frame = cap.read();\n",
    "    #recoloring image \n",
    "    image=cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "   \n",
    "    #Make detection \n",
    "    results = poset.process(image)\n",
    "    #Recolor back to BRG\n",
    "\n",
    "    try: \n",
    "        landmarks=results.pose_landmarks.landmark\n",
    "        shoulders=[landmarks[pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        elbow=[landmarks[pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "        wrist=[landmarks[pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "        arm_angle=calculate_angle(shoulders,elbow,wrist)\n",
    "        print(arm_angle);\n",
    "        cv2.putText(image, str(arm_angle), tuple(np.multiply(elbow, [480, 640]).astype(int)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    #detection rendering \n",
    "    drawing.draw_landmarks(image, results.pose_landmarks , pose.POSE_CONNECTIONS)\n",
    "    cv2.imshow('Mediapipe Feed', image)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for landmrk in pose.PoseLandmark:\n",
    "    print(landmrk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
